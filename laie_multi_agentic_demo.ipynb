{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af86966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing multi-agent system dependencies...\n",
      " openai\n",
      " mcp\n",
      " linkedin-api\n",
      " requests\n",
      " python-dotenv\n",
      " pandas\n",
      " numpy\n",
      " pydantic\n",
      " httpx\n",
      "aiohttp\n",
      " networkx\n",
      " beautifulsoup4\n",
      " langgraph\n",
      " langchain-openai\n",
      " langchain-core\n",
      " tqdm\n",
      " structlog\n",
      "\n",
      " Multi-Agent LAIE System Dependencies Ready!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for multi-agent system\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Core dependencies\n",
    "packages = [\n",
    "    \"openai\",\n",
    "    \"mcp\",\n",
    "    \"linkedin-api\",\n",
    "    \"requests\",\n",
    "    \"python-dotenv\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"pydantic\",\n",
    "    \"httpx\",\n",
    "    \"aiohttp\",\n",
    "    \"networkx\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"langgraph\",\n",
    "    \"langchain-openai\",\n",
    "    \"langchain-core\",\n",
    "    \"tqdm\",\n",
    "    \"structlog\",\n",
    "]\n",
    "\n",
    "print(\"Installing multi-agent system dependencies...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\" {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\" {package}: {e}\")\n",
    "\n",
    "print(\"\\n Multi-Agent LAIE System Dependencies Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4cde6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "import asyncio \n",
    "from datetime import datetime , timedelta\n",
    "from typing import Annotated, Any, Optional, List , Dict , TypedDict, Literal, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import logging\n",
    "import structlog \n",
    "\n",
    "#reuse data models \n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "#load environment variables \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path(\"../.env\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150d9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = structlog.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351d6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Azure openai config \n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AI_FOUNDRY_API_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AI_FOUNDRY_DEPLOYMENT_NAME\", \"gpt-4.1\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AI_FOUNDRY_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "\n",
    "#linkedin data sources \n",
    "LINKEDIN_EMAIL = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "LINKEDIN_PASSWORD = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "LINKEDIN_LI_AT = os.getenv(\"LINKEDIN_LI_AT\")\n",
    "PROXYCURL_API_KEY = os.getenv(\"PROXYCURL_API_KEY\")\n",
    "\n",
    "#Analysis Time window \n",
    "ANALYSIS_START_DATE = datetime(2025, 1, 1)\n",
    "ANALYSIS_END_DATE = datetime(2026, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb61705",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    model=AZURE_OPENAI_DEPLOYMENT,\n",
    "    temperature=0.3,\n",
    "    max_tokens=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7e072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the state schema for langGraph \n",
    "class LAIEState(TypedDict):\n",
    "    \"\"\"State Schema for the LAIE multi-agent system.\"\"\"\n",
    "    #input parameters \n",
    "    public_id : str \n",
    "    data_sources : Dict[str, Any]\n",
    "\n",
    "    # data collection results \n",
    "    raw_profile : Optional[Dict[str, Any]]\n",
    "    raw_posts : Optional[List[Dict[str, Any]]]\n",
    "    data_quality_score : float \n",
    "\n",
    "    # analytics results \n",
    "    monthly_analytics : Optional[List[Dict[str, Any]]]\n",
    "    content_performance : Optional[Dict[str, Any]]\n",
    "    temporal_patterns : Optional[Dict[str, Any]]\n",
    "\n",
    "    # AI generated content \n",
    "    monthly_notes : Optional[List[Dict[str, Any]]]\n",
    "    content_performance : Optional[Dict[str, Any]]\n",
    "    temporal_patterns : Optional[Dict[str, Any]]\n",
    "\n",
    "    # Agent Communication \n",
    "    messages : List[BaseMessage]\n",
    "    current_agent : str \n",
    "    next_agent : Optional[str]\n",
    "\n",
    "    # Error handling \n",
    "    errors : List[str]\n",
    "    retry_count : int \n",
    "\n",
    "    #final output \n",
    "    final_report : Optional[Dict[str, Any]]\n",
    "    audit_trail : List[Dict[str, Any]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6a885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define agent response types \n",
    "class AgentResponse(TypedDict):\n",
    "    success : bool \n",
    "    data : Any \n",
    "    next_agent : Optional[str]\n",
    "    errors : List[str]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa63323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define monthly note structure \n",
    "class MonthlyNote(TypedDict):\n",
    "    month : str \n",
    "    activity_summary : str \n",
    "    key_achievements : List[str]\n",
    "    content_performance : Dict[str, Any]\n",
    "    engagement_highlights : List[str]\n",
    "    recommendations : List[str]\n",
    "    ai_insights : str \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf15971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enums \n",
    "class ContentType(Enum):\n",
    "    TEXT = \"text\"\n",
    "    IMAGE = \"image\"\n",
    "    VIDEO = \"video\"\n",
    "    ARTICLE = \"article\"\n",
    "    CAROUSEL = \"carousel\"\n",
    "    POLL = \"poll\"\n",
    "    DOCUMENT = \"document\"\n",
    "\n",
    "\n",
    "class EngagementType(Enum):\n",
    "    LIKE = \"like\"\n",
    "    COMMENT = \"comment\"\n",
    "    REPOST = \"repost\"\n",
    "    IMPRESSION = \"impression\"\n",
    "\n",
    "#profile models\n",
    "class LinkedInProfile(BaseModel):\n",
    "    user_id : str \n",
    "    full_name : str \n",
    "    headline : str \n",
    "    followers_count : int = Field(default=0)\n",
    "    connections_count : int = Field(default=0)\n",
    "    industry : Optional[str] = Field(default=None)\n",
    "    location : Optional[str] = Field(default=None)\n",
    "    about : Optional[str] = Field(default=None)\n",
    "\n",
    "class LinkedInPost(BaseModel):\n",
    "    post_id : str \n",
    "    user_id : str \n",
    "    content : str \n",
    "    content_type : ContentType \n",
    "    published_at : datetime \n",
    "    likes_count : int = Field(default=0)\n",
    "    comments_count : int = Field(default=0)\n",
    "    reposts_count : int = Field(default=0)\n",
    "    impressions : int = Field(default=0)\n",
    "\n",
    "class MonthlyActivity(BaseModel):\n",
    "    user_id : str \n",
    "    month : str \n",
    "    posts_count : int = Field(default=0)\n",
    "    total_impressions : int = Field(default=0)\n",
    "    total_likes : int = Field(default=0)\n",
    "    engagement_rate : float = Field(default=0.0)\n",
    "    content_types : Dict[str, int] = Field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3e823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngestionAgent:\n",
    "    \"\"\"Agent responsible for collecting LinkedIn data from various sources.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audit_log = []\n",
    "        logger.info(\"IngestionAgent initialized\")\n",
    "    \n",
    "    def process(self, state: LAIEState) -> AgentResponse:\n",
    "        \"\"\"Process data ingestion for the given LinkedIn profile.\"\"\"\n",
    "        public_id = state[\"public_id\"]\n",
    "        data_sources = state[\"data_sources\"]\n",
    "        \n",
    "        logger.info(\"IngestionAgent processing\", public_id=public_id)\n",
    "        \n",
    "        try:\n",
    "            # Attempt data collection\n",
    "            profile, posts = self._collect_data(public_id, data_sources)\n",
    "            \n",
    "            # Validate data quality\n",
    "            quality_score = self._assess_data_quality(profile, posts)\n",
    "            \n",
    "            # Convert to dict format for state\n",
    "            profile_dict = profile.dict() if hasattr(profile, 'dict') else profile\n",
    "            posts_list = [post.dict() if hasattr(post, 'dict') else post for post in posts]\n",
    "            \n",
    "            response = AgentResponse(\n",
    "                success=True,\n",
    "                data={\n",
    "                    \"profile\": profile_dict,\n",
    "                    \"posts\": posts_list,\n",
    "                    \"quality_score\": quality_score\n",
    "                },\n",
    "                message=f\"Successfully collected data for {public_id}\",\n",
    "                next_agent=\"analytics\",\n",
    "                errors=[]\n",
    "            )\n",
    "            \n",
    "            self._log_action(f\"Data ingestion completed for {public_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"IngestionAgent failed\", error=str(e))\n",
    "            response = AgentResponse(\n",
    "                success=False,\n",
    "                data=None,\n",
    "                message=f\"Data ingestion failed: {str(e)}\",\n",
    "                next_agent=None,\n",
    "                errors=[str(e)]\n",
    "            )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _collect_data(self, public_id: str, data_sources: Dict[str, Any]) -> tuple:\n",
    "        \"\"\"Collect data from available sources.\"\"\"\n",
    "        # Priority: GDPR export > Proxycurl > linkedin-api\n",
    "        \n",
    "        if data_sources.get(\"gdpr_export\"):\n",
    "            return self._parse_gdpr_export(data_sources[\"gdpr_export\"], public_id)\n",
    "        \n",
    "        if data_sources.get(\"proxycurl_api_key\"):\n",
    "            return self._fetch_proxycurl_data(public_id, data_sources[\"proxycurl_api_key\"])\n",
    "        \n",
    "        if data_sources.get(\"linkedin_credentials\"):\n",
    "            return self._fetch_linkedin_api_data(public_id, data_sources[\"linkedin_credentials\"])\n",
    "        \n",
    "        raise ValueError(\"No valid data source provided\")\n",
    "    \n",
    "    def _parse_gdpr_export(self, zip_path: str, public_id: str) -> tuple:\n",
    "        \"\"\"Parse GDPR export (simplified implementation).\"\"\"\n",
    "        # In production, this would parse actual GDPR export\n",
    "        # For demo, return mock data\n",
    "        profile = LinkedInProfile(\n",
    "            user_id=public_id,\n",
    "            full_name=\"Demo User\",\n",
    "            headline=\"Professional Title\",\n",
    "            followers_count=1000,\n",
    "            connections_count=500\n",
    "        )\n",
    "        \n",
    "        # Generate mock posts for the analysis period\n",
    "        posts = []\n",
    "        current_date = ANALYSIS_START_DATE\n",
    "        post_id = 0\n",
    "        \n",
    "        while current_date < ANALYSIS_END_DATE:\n",
    "            # Add 2-5 posts per month\n",
    "            posts_this_month = []\n",
    "            num_posts = 2 + (hash(public_id + current_date.strftime(\"%Y-%m\")) % 4)\n",
    "            \n",
    "            for i in range(num_posts):\n",
    "                post_date = current_date + timedelta(days=i * 7)  # Spread posts\n",
    "                if post_date >= ANALYSIS_END_DATE:\n",
    "                    break\n",
    "                \n",
    "                posts.append(LinkedInPost(\n",
    "                    post_id=f\"post_{post_id}\",\n",
    "                    user_id=public_id,\n",
    "                    content=f\"Sample LinkedIn post content for {post_date.strftime('%B %Y')}\",\n",
    "                    content_type=ContentType.TEXT,\n",
    "                    published_at=post_date,\n",
    "                    likes_count=10 + (hash(str(post_id)) % 90),\n",
    "                    comments_count=1 + (hash(str(post_id + 1)) % 9),\n",
    "                    reposts_count=0 + (hash(str(post_id + 2)) % 5),\n",
    "                    impressions=100 + (hash(str(post_id + 3)) % 900)\n",
    "                ))\n",
    "                post_id += 1\n",
    "            \n",
    "            # Move to next month\n",
    "            if current_date.month == 12:\n",
    "                current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "            else:\n",
    "                current_date = current_date.replace(month=current_date.month + 1)\n",
    "        \n",
    "        return profile, posts\n",
    "    \n",
    "    def _fetch_proxycurl_data(self, public_id: str, api_key: str) -> tuple:\n",
    "        \"\"\"Fetch data from Proxycurl API.\"\"\"\n",
    "        import requests\n",
    "        \n",
    "        try:\n",
    "            # Proxycurl API endpoint for profile data\n",
    "            url = \"https://nubela.co/proxycurl/api/v2/linkedin\"\n",
    "            headers = {\n",
    "                'Authorization': f'Bearer {api_key}'\n",
    "            }\n",
    "            params = {\n",
    "                'url': f'https://www.linkedin.com/in/{public_id}',\n",
    "                'fallback_to_cache': 'on-error'\n",
    "            }\n",
    "            \n",
    "            logger.info(\"Fetching profile data from Proxycurl\", public_id=public_id)\n",
    "            response = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract profile information\n",
    "            profile = LinkedInProfile(\n",
    "                user_id=public_id,\n",
    "                full_name=data.get('full_name', f'User {public_id}'),\n",
    "                headline=data.get('headline', data.get('occupation', 'Professional')),\n",
    "                followers_count=data.get('follower_count', 0),\n",
    "                connections_count=data.get('connections', 0),\n",
    "                industry=data.get('industry'),\n",
    "                location=data.get('city', {}).get('full') if data.get('city') else None,\n",
    "                about=data.get('summary')\n",
    "            )\n",
    "            \n",
    "            # Proxycurl doesn't provide posts data, return empty list\n",
    "            posts = []\n",
    "            self._log_action(f\"Successfully fetched Proxycurl data for {public_id}\")\n",
    "            \n",
    "            return profile, posts\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(\"Proxycurl API error\", error=str(e))\n",
    "            raise ValueError(f\"Failed to fetch Proxycurl data: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Proxycurl data processing error\", error=str(e))\n",
    "            raise ValueError(f\"Failed to process Proxycurl data: {str(e)}\")\n",
    "    \n",
    "    def _fetch_linkedin_api_data(self, public_id: str, credentials: Dict) -> tuple:\n",
    "        \"\"\"Fetch data using linkedin-api.\"\"\"\n",
    "        try:\n",
    "            from linkedin_api import Linkedin\n",
    "            \n",
    "            # Initialize LinkedIn client\n",
    "            email = credentials.get(\"email\") or os.getenv(\"LINKEDIN_EMAIL\")\n",
    "            password = credentials.get(\"password\") or os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "            li_at = credentials.get(\"li_at\") or os.getenv(\"LINKEDIN_LI_AT\")\n",
    "            \n",
    "            if li_at:\n",
    "                # Use li_at cookie for authentication\n",
    "                api = Linkedin(\"\", \"\", cookies={\"li_at\": li_at})\n",
    "            elif email and password:\n",
    "                # Use email/password authentication\n",
    "                api = Linkedin(email, password)\n",
    "            else:\n",
    "                raise ValueError(\"No valid LinkedIn credentials provided\")\n",
    "            \n",
    "            logger.info(\"Fetching profile data from LinkedIn API\", public_id=public_id)\n",
    "            \n",
    "            # Get profile data\n",
    "            profile_data = api.get_profile(public_id)\n",
    "            \n",
    "            # Extract profile information\n",
    "            profile = LinkedInProfile(\n",
    "                user_id=public_id,\n",
    "                full_name=profile_data.get(\"firstName\", \"\") + \" \" + profile_data.get(\"lastName\", \"\") if profile_data.get(\"firstName\") else f\"User {public_id}\",\n",
    "                headline=profile_data.get(\"headline\", \"Professional\"),\n",
    "                followers_count=profile_data.get(\"followerCount\", 0),\n",
    "                connections_count=profile_data.get(\"connectionsCount\", 0),\n",
    "                industry=profile_data.get(\"industryName\"),\n",
    "                location=profile_data.get(\"locationName\"),\n",
    "                about=profile_data.get(\"summary\")\n",
    "            )\n",
    "            \n",
    "            # Get posts/activity data\n",
    "            logger.info(\"Fetching posts data from LinkedIn API\", public_id=public_id)\n",
    "            urn_id = profile_data.get(\"public_id\") or public_id\n",
    "            \n",
    "            # Get recent posts (last 365 days)\n",
    "            posts_data = api.get_profile_posts(urn_id, post_count=50)  # Get up to 50 recent posts\n",
    "            posts = []\n",
    "            \n",
    "            for post_item in posts_data.get(\"elements\", []):\n",
    "                post = post_item.get(\"update\", {}).get(\"share\", {})\n",
    "                if not post:\n",
    "                    continue\n",
    "                \n",
    "                # Extract post information\n",
    "                post_id = str(post.get(\"urn\", \"\").split(\":\")[-1])\n",
    "                content = post.get(\"text\", {}).get(\"text\", \"\")\n",
    "                published_at_str = post.get(\"created\", {}).get(\"time\")\n",
    "                \n",
    "                # Convert timestamp to datetime\n",
    "                if published_at_str:\n",
    "                    try:\n",
    "                        published_at = datetime.fromtimestamp(int(published_at_str) / 1000)\n",
    "                    except:\n",
    "                        published_at = ANALYSIS_START_DATE\n",
    "                else:\n",
    "                    published_at = ANALYSIS_START_DATE\n",
    "                \n",
    "                # Skip posts outside analysis window\n",
    "                if published_at < ANALYSIS_START_DATE or published_at >= ANALYSIS_END_DATE:\n",
    "                    continue\n",
    "                \n",
    "                # Get engagement metrics\n",
    "                social_counts = post.get(\"socialDetail\", {}).get(\"totalSocialActivityCounts\", {})\n",
    "                likes_count = social_counts.get(\"numLikes\", 0)\n",
    "                comments_count = social_counts.get(\"numComments\", 0)\n",
    "                reposts_count = social_counts.get(\"numShares\", 0)\n",
    "                impressions = social_counts.get(\"numImpressions\", 0)\n",
    "                \n",
    "                # Determine content type\n",
    "                content_type = ContentType.TEXT\n",
    "                if post.get(\"content\", {}).get(\"images\"):\n",
    "                    content_type = ContentType.IMAGE\n",
    "                elif post.get(\"content\", {}).get(\"videos\"):\n",
    "                    content_type = ContentType.VIDEO\n",
    "                \n",
    "                posts.append(LinkedInPost(\n",
    "                    post_id=post_id,\n",
    "                    user_id=public_id,\n",
    "                    content=content,\n",
    "                    content_type=content_type,\n",
    "                    published_at=published_at,\n",
    "                    likes_count=likes_count,\n",
    "                    comments_count=comments_count,\n",
    "                    reposts_count=reposts_count,\n",
    "                    impressions=impressions\n",
    "                ))\n",
    "            \n",
    "            self._log_action(f\"Successfully fetched LinkedIn API data for {public_id}: {len(posts)} posts\")\n",
    "            \n",
    "            return profile, posts\n",
    "            \n",
    "        except ImportError as e:\n",
    "            logger.error(\"linkedin-api package not installed\", error=str(e))\n",
    "            raise ValueError(\"linkedin-api package required for LinkedIn API data fetching\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"LinkedIn API error\", error=str(e))\n",
    "            raise ValueError(f\"Failed to fetch LinkedIn API data: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f60aea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyticsAgent:\n",
    "    \"\"\"Agent responsible for performing deterministic analytics on LinkedIn data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audit_log = []\n",
    "        logger.info(\"AnalyticsAgent initialized\")\n",
    "    \n",
    "    def process(self, state: LAIEState) -> AgentResponse:\n",
    "        \"\"\"Process analytics on the collected LinkedIn data.\"\"\"\n",
    "        logger.info(\"AnalyticsAgent processing\")\n",
    "        \n",
    "        try:\n",
    "            # Extract data from state\n",
    "            profile_data = state.get(\"raw_profile\")\n",
    "            posts_data = state.get(\"raw_posts\", [])\n",
    "            \n",
    "            if not profile_data or not posts_data:\n",
    "                raise ValueError(\"Insufficient data for analytics\")\n",
    "            \n",
    "            # Convert to model objects\n",
    "            profile = LinkedInProfile(**profile_data)\n",
    "            posts = [LinkedInPost(**post_data) for post_data in posts_data]\n",
    "            \n",
    "            # Perform analytics\n",
    "            monthly_analytics = self._compute_monthly_analytics(profile, posts)\n",
    "            content_performance = self._compute_content_performance(posts)\n",
    "            temporal_patterns = self._compute_temporal_patterns(posts)\n",
    "            \n",
    "            response = AgentResponse(\n",
    "                success=True,\n",
    "                data={\n",
    "                    \"monthly_analytics\": [ma.dict() if hasattr(ma, 'dict') else ma for ma in monthly_analytics],\n",
    "                    \"content_performance\": content_performance,\n",
    "                    \"temporal_patterns\": temporal_patterns\n",
    "                },\n",
    "                message=\"Analytics computation completed successfully\",\n",
    "                next_agent=\"monthly_analysis\",\n",
    "                errors=[]\n",
    "            )\n",
    "            \n",
    "            self._log_action(\"Analytics computation completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"AnalyticsAgent failed\", error=str(e))\n",
    "            response = AgentResponse(\n",
    "                success=False,\n",
    "                data=None,\n",
    "                message=f\"Analytics computation failed: {str(e)}\",\n",
    "                next_agent=None,\n",
    "                errors=[str(e)]\n",
    "            )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _compute_monthly_analytics(self, profile: LinkedInProfile, posts: List[LinkedInPost]) -> List[MonthlyActivity]:\n",
    "        \"\"\"Compute monthly activity analytics.\"\"\"\n",
    "        monthly_data = defaultdict(lambda: {\n",
    "            \"posts_count\": 0,\n",
    "            \"total_impressions\": 0,\n",
    "            \"total_likes\": 0,\n",
    "            \"total_comments\": 0,\n",
    "            \"total_reposts\": 0,\n",
    "            \"content_types\": defaultdict(int)\n",
    "        })\n",
    "        \n",
    "        for post in posts:\n",
    "            month_key = post.published_at.strftime(\"%Y-%m\")\n",
    "            month_data = monthly_data[month_key]\n",
    "            \n",
    "            month_data[\"posts_count\"] += 1\n",
    "            month_data[\"total_impressions\"] += post.impressions\n",
    "            month_data[\"total_likes\"] += post.likes_count\n",
    "            month_data[\"total_comments\"] += post.comments_count\n",
    "            month_data[\"total_reposts\"] += post.reposts_count\n",
    "            month_data[\"content_types\"][post.content_type.value] += 1\n",
    "        \n",
    "        # Convert to MonthlyActivity objects\n",
    "        monthly_activities = []\n",
    "        for month_key, data in sorted(monthly_data.items()):\n",
    "            total_engagements = data[\"total_likes\"] + data[\"total_comments\"] + data[\"total_reposts\"]\n",
    "            engagement_rate = total_engagements / data[\"total_impressions\"] if data[\"total_impressions\"] > 0 else 0\n",
    "            \n",
    "            monthly_activities.append(MonthlyActivity(\n",
    "                user_id=profile.user_id,\n",
    "                month=month_key,\n",
    "                posts_count=data[\"posts_count\"],\n",
    "                total_impressions=data[\"total_impressions\"],\n",
    "                total_likes=data[\"total_likes\"],\n",
    "                engagement_rate=engagement_rate,\n",
    "                content_types=dict(data[\"content_types\"])\n",
    "            ))\n",
    "        \n",
    "        return monthly_activities\n",
    "    \n",
    "    def _compute_content_performance(self, posts: List[LinkedInPost]) -> Dict[str, Any]:\n",
    "        \"\"\"Compute content type performance analytics.\"\"\"\n",
    "        if not posts:\n",
    "            return {}\n",
    "        \n",
    "        content_stats = defaultdict(lambda: {\n",
    "            \"count\": 0,\n",
    "            \"total_impressions\": 0,\n",
    "            \"total_engagements\": 0,\n",
    "            \"avg_impressions\": 0,\n",
    "            \"avg_engagements\": 0,\n",
    "            \"engagement_rate\": 0\n",
    "        })\n",
    "        \n",
    "        for post in posts:\n",
    "            ct = post.content_type.value\n",
    "            stats = content_stats[ct]\n",
    "            \n",
    "            stats[\"count\"] += 1\n",
    "            stats[\"total_impressions\"] += post.impressions\n",
    "            stats[\"total_engagements\"] += post.likes_count + post.comments_count + post.reposts_count\n",
    "        \n",
    "        # Calculate averages\n",
    "        for ct, stats in content_stats.items():\n",
    "            if stats[\"count\"] > 0:\n",
    "                stats[\"avg_impressions\"] = stats[\"total_impressions\"] / stats[\"count\"]\n",
    "                stats[\"avg_engagements\"] = stats[\"total_engagements\"] / stats[\"count\"]\n",
    "                stats[\"engagement_rate\"] = stats[\"total_engagements\"] / stats[\"total_impressions\"] if stats[\"total_impressions\"] > 0 else 0\n",
    "        \n",
    "        # Find best performing type\n",
    "        best_type = max(content_stats.items(), key=lambda x: x[1][\"avg_impressions\"]) if content_stats else None\n",
    "        \n",
    "        return {\n",
    "            \"content_stats\": dict(content_stats),\n",
    "            \"best_performing_type\": best_type[0] if best_type else None,\n",
    "            \"total_posts_analyzed\": len(posts)\n",
    "        }\n",
    "    \n",
    "    def _compute_temporal_patterns(self, posts: List[LinkedInPost]) -> Dict[str, Any]:\n",
    "        \"\"\"Compute temporal posting patterns.\"\"\"\n",
    "        if not posts:\n",
    "            return {}\n",
    "        \n",
    "        # Analyze posting patterns\n",
    "        posts_by_weekday = defaultdict(int)\n",
    "        posts_by_hour = defaultdict(int)\n",
    "        posts_by_month = defaultdict(int)\n",
    "        \n",
    "        for post in posts:\n",
    "            posts_by_weekday[post.published_at.weekday()] += 1\n",
    "            posts_by_hour[post.published_at.hour] += 1\n",
    "            posts_by_month[post.published_at.strftime(\"%Y-%m\")] += 1\n",
    "        \n",
    "        # Calculate posting consistency\n",
    "        total_days = (ANALYSIS_END_DATE - ANALYSIS_START_DATE).days\n",
    "        active_days = len(set(post.published_at.date() for post in posts))\n",
    "        posting_consistency = active_days / total_days if total_days > 0 else 0\n",
    "        \n",
    "        # Find optimal posting times\n",
    "        best_weekday = max(posts_by_weekday.items(), key=lambda x: x[1])[0] if posts_by_weekday else None\n",
    "        best_hour = max(posts_by_hour.items(), key=lambda x: x[1])[0] if posts_by_hour else None\n",
    "        \n",
    "        weekday_names = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "        \n",
    "        return {\n",
    "            \"posting_consistency\": posting_consistency,\n",
    "            \"active_days\": active_days,\n",
    "            \"total_days\": total_days,\n",
    "            \"best_posting_weekday\": weekday_names[best_weekday] if best_weekday is not None else None,\n",
    "            \"best_posting_hour\": best_hour,\n",
    "            \"posts_by_month\": dict(posts_by_month),\n",
    "            \"avg_posts_per_day\": len(posts) / total_days if total_days > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _log_action(self, action: str):\n",
    "        \"\"\"Log agent actions.\"\"\"\n",
    "        timestamp = datetime.utcnow().isoformat()\n",
    "        self.audit_log.append({\"timestamp\": timestamp, \"action\": action, \"agent\": \"analytics\"})\n",
    "        print(f\" AnalyticsAgent: {action}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "241a1980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-20 23:27:40 [info     ] AnalyticsAgent initialized    \n"
     ]
    }
   ],
   "source": [
    "# Initialize analytics agent\n",
    "analytics_agent = AnalyticsAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d544fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly analysis agent \n",
    "class MonthlyAnalysisAgent:\n",
    "    \"\"\"Agent responsible for creating detailed month-wise activity analysis using AI \"\"\"\n",
    "    def __init__(self):\n",
    "        self.audit_log = []\n",
    "        logger.info(\"MonthlyAnalysisAgent initialized\")\n",
    "\n",
    "    def process(self, state : LAIEState) -> AgentResponse:\n",
    "        \"\"\"Process monthly analysis on the collected LinkedIn data.\"\"\"\n",
    "        logger.info(\"MonthlyAnalysisAgent processing\")\n",
    "        \n",
    "        try : \n",
    "            monthly_analytics = state.get(\"monthly_analytics\",[])\n",
    "            profile_data = state.get(\"raw_profile\",{})\n",
    "\n",
    "            if not monthly_analytics:\n",
    "                raise ValueError(\"No monthly analytics data available\")\n",
    "\n",
    "            monthly_notes = []\n",
    "            for month_data in monthly_analytics:\n",
    "                note = self._generate_monthly_note(month_data, profile_data)\n",
    "                monthly_notes.append(note)\n",
    "            \n",
    "            response = AgentResponse(\n",
    "                success=True,\n",
    "                data=monthly_notes,\n",
    "                message=f\"Generated {len(monthly_notes)} monthly activity notes\",\n",
    "                next_agent=\"summary\",\n",
    "                \n",
    "            )\n",
    "            self._log_action(\"Monthly analysis completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"MonthlyAnalysisAgent failed\", error=str(e))\n",
    "            response = AgentResponse(\n",
    "                success=False,\n",
    "                data=None,  \n",
    "                message=f\"Monthly analysis failed: {str(e)}\",\n",
    "                next_agent=None,\n",
    "                errors=[str(e)]\n",
    "            )\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def _generate_montly_note(self, month_data : Dict[str, Any], profile_data : Dict[str,Any]) -> MonthlyNote:\n",
    "        \"\"\"Generate a detailed monthly note using AI.\"\"\"\n",
    "        month = month_data.get(\"month\")\n",
    "        posts_count = month_data.get(\"posts_count\")\n",
    "        impressions = month_data.get(\"total_impressions\",0)\n",
    "        likes = month_data.get(\"total_likes\",0)\n",
    "        engagement_rate = month_data.get(\"engagement_rate\",0)\n",
    "        content_types  = month_data.get(\"content_types\",{})\n",
    "        profile_name = profile_data.get(\"full_name\",\"\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
